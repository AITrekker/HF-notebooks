{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformers 설치 방법\n",
    "! pip install transformers datasets evaluate accelerate\n",
    "# 마지막 릴리스 대신 소스에서 설치하려면, 위 명령을 주석으로 바꾸고 아래 명령을 해제하세요.\n",
    "# ! pip install git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 키포인트 탐지 [[keypoint-detection]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "키포인트 감지(Keypoint detection)은 이미지 내의 특정 포인트를 식별하고 위치를 탐지합니다. 이러한 키포인트는 랜드마크라고도 불리며 얼굴 특징이나 물체의 일부와 같은 의미 있는 특징을 나타냅니다.\n",
    "키포인트 감지 모델들은 이미지를 입력으로 받아 아래와 같은 출력을 반환합니다.\n",
    "\n",
    "- **키포인트들과 점수**: 관심 포인트들과 해당 포인트에 대한 신뢰도 점수\n",
    "- **디스크립터(Descriptors)**: 각 키포인트를 둘러싼 이미지 영역의 표현으로 텍스처, 그라데이션, 방향 및 기타 속성을 캡처합니다.\n",
    "\n",
    "이번 가이드에서는 이미지에서 키포인트를 추출하는 방법을 다루어 보겠습니다.\n",
    "\n",
    "이번 튜토리얼에서는 키포인트 감지의 기본이 되는 모델인 [SuperPoint](https://huggingface.co/docs/transformers/main/ko/tasks/./model_doc/superpoint)를 사용해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, SuperPointForKeypointDetection\n",
    "processor = AutoImageProcessor.from_pretrained(\"magic-leap-community/superpoint\")\n",
    "model = SuperPointForKeypointDetection.from_pretrained(\"magic-leap-community/superpoint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래의 이미지로 모델을 테스트 해보겠습니다.\n",
    "\n",
    "<div style=\"display: flex; align-items: center;\">\n",
    "    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg\" \n",
    "         alt=\"Bee\" \n",
    "         style=\"height: 200px; object-fit: contain; margin-right: 10px;\">\n",
    "    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/cats.png\" \n",
    "         alt=\"Cats\" \n",
    "         style=\"height: 200px; object-fit: contain;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "import cv2\n",
    "\n",
    "\n",
    "url_image_1 = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg\"\n",
    "image_1 = Image.open(requests.get(url_image_1, stream=True).raw)\n",
    "url_image_2 = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/cats.png\"\n",
    "image_2 = Image.open(requests.get(url_image_2, stream=True).raw)\n",
    "\n",
    "images = [image_1, image_2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 입력을 처리하고 추론을 할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(images,return_tensors=\"pt\").to(model.device, model.dtype)\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 출력에는 배치 내의 각 항목에 대한 상대적인 키포인트, 디스크립터, 마스크와 점수가 있습니다. 마스크는 이미지에서 키포인트가 있는 영역을 강조하는 역할을 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SuperPointKeypointDescriptionOutput(loss=None, keypoints=tensor([[[0.0437, 0.0167],\n",
    "         [0.0688, 0.0167],\n",
    "         [0.0172, 0.0188],\n",
    "         ...,\n",
    "         [0.5984, 0.9812],\n",
    "         [0.6953, 0.9812]]]), \n",
    "         scores=tensor([[0.0056, 0.0053, 0.0079,  ..., 0.0125, 0.0539, 0.0377],\n",
    "        [0.0206, 0.0058, 0.0065,  ..., 0.0000, 0.0000, 0.0000]],\n",
    "       grad_fn=<CopySlices>), descriptors=tensor([[[-0.0807,  0.0114, -0.1210,  ..., -0.1122,  0.0899,  0.0357],\n",
    "         [-0.0807,  0.0114, -0.1210,  ..., -0.1122,  0.0899,  0.0357],\n",
    "         [-0.0807,  0.0114, -0.1210,  ..., -0.1122,  0.0899,  0.0357],\n",
    "         ...],\n",
    "       grad_fn=<CopySlices>), mask=tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
    "        [1, 1, 1,  ..., 0, 0, 0]], dtype=torch.int32), hidden_states=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이미지에 실제 키포인트를 표시하기 위해선 결과값을 후처리 해야합니다. 이를 위해 실제 이미지 크기를 결과값과 함께 `post_process_keypoint_detection`에 전달해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_sizes = [(image.size[1], image.size[0]) for image in images]\n",
    "outputs = processor.post_process_keypoint_detection(outputs, image_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 코드를 통해 결과값은 딕셔너리를 갖는 리스트가 되고, 각 딕셔너리들은 후처리된 키포인트, 점수 및 디스크립터로 이루어져있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[{'keypoints': tensor([[ 226,   57],\n",
    "          [ 356,   57],\n",
    "          [  89,   64],\n",
    "          ...,\n",
    "          [3604, 3391]], dtype=torch.int32),\n",
    "  'scores': tensor([0.0056, 0.0053, ...], grad_fn=<IndexBackward0>),\n",
    "  'descriptors': tensor([[-0.0807,  0.0114, -0.1210,  ..., -0.1122,  0.0899,  0.0357],\n",
    "          [-0.0807,  0.0114, -0.1210,  ..., -0.1122,  0.0899,  0.0357]],\n",
    "         grad_fn=<IndexBackward0>)},\n",
    "    {'keypoints': tensor([[ 46,   6],\n",
    "          [ 78,   6],\n",
    "          [422,   6],\n",
    "          [206, 404]], dtype=torch.int32),\n",
    "  'scores': tensor([0.0206, 0.0058, 0.0065, 0.0053, 0.0070, ...,grad_fn=<IndexBackward0>),\n",
    "  'descriptors': tensor([[-0.0525,  0.0726,  0.0270,  ...,  0.0389, -0.0189, -0.0211],\n",
    "          [-0.0525,  0.0726,  0.0270,  ...,  0.0389, -0.0189, -0.0211]}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 위 딕셔너리를 사용하여 키포인트를 표시할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "for i in range(len(images)):\n",
    "  keypoints = outputs[i][\"keypoints\"]\n",
    "  scores = outputs[i][\"scores\"]\n",
    "  descriptors = outputs[i][\"descriptors\"]\n",
    "  keypoints = outputs[i][\"keypoints\"].detach().numpy()\n",
    "  scores = outputs[i][\"scores\"].detach().numpy()\n",
    "  image = images[i]\n",
    "  image_width, image_height = image.size\n",
    "\n",
    "  plt.axis('off')\n",
    "  plt.imshow(image)\n",
    "  plt.scatter(\n",
    "      keypoints[:, 0],\n",
    "      keypoints[:, 1],\n",
    "      s=scores * 100,\n",
    "      c='cyan',\n",
    "      alpha=0.4\n",
    "  )\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래에서 결과를 확인할 수 있습니다.\n",
    "\n",
    "<div style=\"display: flex; align-items: center;\">\n",
    "    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee_keypoint.png\" \n",
    "         alt=\"Bee\" \n",
    "         style=\"height: 200px; object-fit: contain; margin-right: 10px;\">\n",
    "    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/cats_keypoint.png\" \n",
    "         alt=\"Cats\" \n",
    "         style=\"height: 200px; object-fit: contain;\">\n",
    "</div>"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
