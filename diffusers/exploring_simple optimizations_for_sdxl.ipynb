{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VpOioQ7TfxOz"
   },
   "source": [
    "## Exploring simple optimizations for Stable Diffusion XL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0c6FCLrjKR5k"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WxpxpPruKD6o"
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/huggingface/diffusers -q\n",
    "!pip install transformers accelerate -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zK5N3pJHfZX9"
   },
   "source": [
    "## Unoptimized setup\n",
    "\n",
    "* FP32 computation\n",
    "* Default attention processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H6ipTyxQKML3"
   },
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionXLPipeline\n",
    "\n",
    "pipe = StableDiffusionXLPipeline.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\")\n",
    "pipe = pipe.to(\"cuda\")\n",
    "pipe.unet.set_default_attn_processor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FlPxa9diNVsx"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "\n",
    "num_iterations = 3\n",
    "num_inference_steps = 25\n",
    "prompt = \"a photo of an astronaut riding a horse on mars\"\n",
    "num_images_per_prompt = 4\n",
    "\n",
    "\n",
    "def bytes_to_giga_bytes(bytes):\n",
    "    return bytes / 1024 / 1024 / 1024\n",
    "\n",
    "\n",
    "def timeit(\n",
    "    pipeline,\n",
    "    prompt_embeds=None,\n",
    "    negative_prompt_embeds=None,\n",
    "    pooled_prompt_embeds=None,\n",
    "    negative_pooled_prompt_embeds=None,\n",
    "):\n",
    "    if prompt_embeds is None:\n",
    "        call_args = dict(\n",
    "            prompt=prompt,\n",
    "            num_images_per_prompt=num_images_per_prompt,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "        )\n",
    "    else:\n",
    "        call_args = dict(\n",
    "            prompt_embeds=prompt_embeds,\n",
    "            negative_prompt_embeds=negative_prompt_embeds,\n",
    "            pooled_prompt_embeds=pooled_prompt_embeds,\n",
    "            negative_pooled_prompt_embeds=negative_pooled_prompt_embeds,\n",
    "            num_images_per_prompt=num_images_per_prompt,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "        )\n",
    "    for i in range(num_iterations):\n",
    "        start = time.time_ns()\n",
    "        _ = pipeline(**call_args)\n",
    "        end = time.time_ns()\n",
    "        if i == num_iterations - 1:\n",
    "            print(f\"Execution time -- {(end - start) / 1e6:.1f} ms\\n\")\n",
    "    print(\n",
    "        f\"Max memory allocated: {bytes_to_giga_bytes(torch.cuda.max_memory_allocated())} GB\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pN-9CQuXQBGV"
   },
   "outputs": [],
   "source": [
    "timeit(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yx1GizAMSLAg"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "def flush():\n",
    "  gc.collect()\n",
    "  torch.cuda.empty_cache()\n",
    "  torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "del pipe\n",
    "flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "05Q6wQ9Ffh_Q"
   },
   "source": [
    "## Just FP16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CSXXwbNJSQbH"
   },
   "outputs": [],
   "source": [
    "pipe = StableDiffusionXLPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16\n",
    ")\n",
    "pipe = pipe.to(\"cuda\")\n",
    "pipe.unet.set_default_attn_processor()\n",
    "\n",
    "timeit(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hTiHMZBGULvF"
   },
   "outputs": [],
   "source": [
    "del pipe\n",
    "flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CdgoEbJ2fqT4"
   },
   "source": [
    "## FP16 + SDPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2jExLo7IXert"
   },
   "outputs": [],
   "source": [
    "pipe = StableDiffusionXLPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16\n",
    ")\n",
    "pipe = pipe.to(\"cuda\")\n",
    "\n",
    "timeit(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L_lBhfSdYTOd"
   },
   "outputs": [],
   "source": [
    "del pipe\n",
    "flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r-uc8PqtU3Vj"
   },
   "source": [
    "From here on, we refer to \"FP16 + SDPA\" as the default setting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nBHYf6WTf5D-"
   },
   "source": [
    "## Default + `torch.compile()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iQ9V76aeYqjU"
   },
   "outputs": [],
   "source": [
    "pipe = StableDiffusionXLPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16\n",
    ")\n",
    "pipe = pipe.to(\"cuda\")\n",
    "pipe.unet = torch.compile(pipe.unet, mode=\"reduce-overhead\", fullgraph=True)\n",
    "\n",
    "timeit(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W1MDkID1a_eU"
   },
   "outputs": [],
   "source": [
    "del pipe\n",
    "flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fvIDIm-ff9AW"
   },
   "source": [
    "## Default + Model CPU Offloading\n",
    "\n",
    "Here we focus more on the memory optimization rather than inference speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "baM5rtmxcXOc"
   },
   "outputs": [],
   "source": [
    "pipe = StableDiffusionXLPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16\n",
    ")\n",
    "pipe.enable_model_cpu_offload()\n",
    "\n",
    "timeit(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AI5V68_gdJ5T"
   },
   "outputs": [],
   "source": [
    "del pipe\n",
    "flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HAqWhXOrgFGr"
   },
   "source": [
    "## Default + Sequential CPU Offloading\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kGWRyvcidKT5"
   },
   "outputs": [],
   "source": [
    "pipe = StableDiffusionXLPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16\n",
    ")\n",
    "pipe.enable_sequential_cpu_offload()\n",
    "\n",
    "timeit(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zSy6UWV6eYhS"
   },
   "outputs": [],
   "source": [
    "del pipe\n",
    "flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7t9pbR1PgT-z"
   },
   "source": [
    "## Default + VAE Slicing\n",
    "\n",
    "Specifically suited for optimizing memory for decoding latents into higher-res images without compromising too much on the inference speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OcIeNb6CeZWi"
   },
   "outputs": [],
   "source": [
    "pipe = StableDiffusionXLPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16\n",
    ")\n",
    "pipe = pipe.to(\"cuda\")\n",
    "pipe.enable_vae_slicing()\n",
    "\n",
    "timeit(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4XAnHrc4jPZS"
   },
   "outputs": [],
   "source": [
    "del pipe\n",
    "flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FP4Qo_Svi8i7"
   },
   "source": [
    "## Default + VAE Slicing + Sequential CPU Offloading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vK49SwlTi64T"
   },
   "outputs": [],
   "source": [
    "pipe = StableDiffusionXLPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16\n",
    ")\n",
    "pipe.enable_sequential_cpu_offload()\n",
    "pipe.enable_vae_slicing()\n",
    "\n",
    "timeit(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZVwDZCDcjQBG"
   },
   "outputs": [],
   "source": [
    "del pipe\n",
    "flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qEq-4plZiCOZ"
   },
   "source": [
    "## Default + Precompting text embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ptkitpYusV9"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import CLIPTextModel, CLIPTextModelWithProjection, CLIPTokenizer\n",
    "\n",
    "\n",
    "pipe_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "torch_dtype = torch.float16\n",
    "\n",
    "# Load the text encoders and tokenizers.\n",
    "text_encoder = CLIPTextModel.from_pretrained(pipe_id, subfolder=\"text_encoder\", torch_dtype=torch.float16).to(\"cuda\")\n",
    "tokenizer = CLIPTokenizer.from_pretrained(pipe_id, subfolder=\"tokenizer\")\n",
    "text_encoder_2 = CLIPTextModelWithProjection.from_pretrained(pipe_id, subfolder=\"text_encoder_2\", torch_dtype=torch.float16).to(\"cuda\")\n",
    "tokenizer_2 = CLIPTokenizer.from_pretrained(pipe_id, subfolder=\"tokenizer_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "glaz8SYOBdG9"
   },
   "outputs": [],
   "source": [
    "def encode_prompt(tokenizers, text_encoders, prompt: str, negative_prompt: str = None):\n",
    "    device = text_encoders[0].device\n",
    "\n",
    "    if isinstance(prompt, str):\n",
    "        prompt = [prompt]\n",
    "    batch_size = len(prompt)\n",
    "\n",
    "    prompt_embeds_list = []\n",
    "    for tokenizer, text_encoder in zip(tokenizers, text_encoders):\n",
    "        text_inputs = tokenizer(\n",
    "            prompt,\n",
    "            padding=\"max_length\",\n",
    "            max_length=tokenizer.model_max_length,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        text_input_ids = text_inputs.input_ids\n",
    "\n",
    "        prompt_embeds = text_encoder(text_input_ids.to(device), output_hidden_states=True)\n",
    "        pooled_prompt_embeds = prompt_embeds[0]\n",
    "        prompt_embeds = prompt_embeds.hidden_states[-2]\n",
    "        prompt_embeds_list.append(prompt_embeds)\n",
    "\n",
    "    prompt_embeds = torch.concat(prompt_embeds_list, dim=-1)\n",
    "\n",
    "    if negative_prompt is None:\n",
    "        negative_prompt_embeds = torch.zeros_like(prompt_embeds)\n",
    "        negative_pooled_prompt_embeds = torch.zeros_like(pooled_prompt_embeds)\n",
    "    else:\n",
    "        negative_prompt = batch_size * [negative_prompt] if isinstance(negative_prompt, str) else negative_prompt\n",
    "\n",
    "        negative_prompt_embeds_list = []\n",
    "        for tokenizer, text_encoder in zip(tokenizers, text_encoders):\n",
    "            uncond_input = tokenizer(\n",
    "                negative_prompt,\n",
    "                padding=\"max_length\",\n",
    "                max_length=tokenizer.model_max_length,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "            negative_prompt_embeds = text_encoder(uncond_input.input_ids.to(device), output_hidden_states=True)\n",
    "            negative_pooled_prompt_embeds = negative_prompt_embeds[0]\n",
    "            negative_prompt_embeds = negative_prompt_embeds.hidden_states[-2]\n",
    "            negative_prompt_embeds_list.append(negative_prompt_embeds)\n",
    "\n",
    "        negative_prompt_embeds = torch.concat(negative_prompt_embeds_list, dim=-1)\n",
    "\n",
    "    return prompt_embeds, negative_prompt_embeds, pooled_prompt_embeds, negative_pooled_prompt_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kq2mquG4BnuV"
   },
   "outputs": [],
   "source": [
    "tokenizers = [tokenizer, tokenizer_2]\n",
    "text_encoders = [text_encoder, text_encoder_2]\n",
    "\n",
    "(\n",
    "    prompt_embeds,\n",
    "    negative_prompt_embeds,\n",
    "    pooled_prompt_embeds,\n",
    "    negative_pooled_prompt_embeds\n",
    ") = encode_prompt(tokenizers, text_encoders, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rMKZs2CiVwUT"
   },
   "outputs": [],
   "source": [
    "del text_encoder, text_encoder_2, tokenizer, tokenizer_2\n",
    "flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z5Nr3sNCsfG6"
   },
   "outputs": [],
   "source": [
    "pipe = StableDiffusionXLPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    text_encoder=None,\n",
    "    text_encoder_2=None,\n",
    "    tokenizer=None,\n",
    "    tokenizer_2=None,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "pipe = pipe.to(\"cuda\")\n",
    "\n",
    "timeit(\n",
    "    pipe,\n",
    "    prompt_embeds,\n",
    "    negative_prompt_embeds,\n",
    "    pooled_prompt_embeds,\n",
    "    negative_pooled_prompt_embeds,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rpj9XGYZVrR-"
   },
   "outputs": [],
   "source": [
    "del pipe\n",
    "flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZgGSeJ79hOF4"
   },
   "source": [
    "## Default + Tiny Autoencoder\n",
    "\n",
    "This is better suited for generating (almost) instant previews. The \"instant\" part is of course, GPU-dependent. On an A10G, for example, it can be achieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RU4qIszCefNK"
   },
   "outputs": [],
   "source": [
    "from diffusers import AutoencoderTiny\n",
    "\n",
    "pipe = StableDiffusionXLPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16\n",
    ")\n",
    "pipe.vae = AutoencoderTiny.from_pretrained(\"madebyollin/taesdxl\", torch_dtype=torch.float16)\n",
    "pipe = pipe.to(\"cuda\")\n",
    "\n",
    "timeit(pipe)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (HF-notebooks)",
   "language": "python",
   "name": "HF-notebooks"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
